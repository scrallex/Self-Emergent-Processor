# Combinatorial Information Growth vs Logarithmic Energy Scaling

## Introduction  
Information capacity in many systems grows *combinatorially* (often exponentially) with each additional unit, whereas the energy or physical resources required per added unit tends to increase much more slowly (often linearly or logarithmically). This disparity means a small increase in system size can yield a vast increase in possible states or complexity. We explore this principle across four domains: (1) quantum mechanics and information theory, (2) AI scaling laws, (3) biological intelligence, and (4) physics and entropy. Each section examines how adding discrete units (quantum states, neural network parameters, neurons, etc.) explodes the information/complexity, and how energy requirements scale sub-linearly by comparison. We also cite theoretical and experimental research supporting these ideas, as well as contrasting viewpoints that highlight limitations or nuances.

## Quantum Mechanics and Information Theory  
Quantum systems illustrate combinatorial information growth clearly. **In quantum information**, the state space size grows exponentially with the number of particles or qubits. A quantum register of *n* qubits can exist in a superposition of $2^n$ basis states simultaneously ([Superposition and entanglement - Quantum Inspire](https://www.quantum-inspire.com/kbase/superposition-and-entanglement/#:~:text=Quantum%20superposition%20is%20fundamentally%20different,of%20quantum%20states%20is%20exponential)). In contrast, combining *n* classical bits yields only *n* possible additive states (or $2^n$ possible bit strings but only one at a time). This means each additional qubit **doubles** the computational state space, an enormous combinatorial expansion. For example, describing the joint state of 1000 quantum bits would formally require specifying $2^{1000}$ complex amplitudes ([PHYS771 Lecture 13: How Big are Quantum States?](https://www.scottaaronson.com/democritus/lec13.html#:~:text=On%20the%20Bayesian%20view%2C%20a,those%20outcomes%20as%20physically%20real)) – a number of configurations vastly exceeding what is classically tractable.

Such exponential information capacity from quantized units underpins emergent complexity in quantum systems. *Entanglement* is a prime example: when particles become entangled, their combined state cannot be factored into independent parts, leading to richer, non-classical correlations. These emergent properties arise from the combinatorial possibilities of quantum states. The challenge, however, is that accessing or observing all that information is non-trivial due to quantum measurement constraints – we cannot directly read out $2^n$ bits from *n* qubits. Nevertheless, quantum computers leverage this huge state space through interference, achieving computations (like factoring large numbers) faster than any classical method by effectively exploring many configurations in parallel ([PHYS771 Lecture 13: How Big are Quantum States?](https://www.scottaaronson.com/democritus/lec13.html#:~:text=On%20the%20Bayesian%20view%2C%20a,those%20outcomes%20as%20physically%20real)) ([PHYS771 Lecture 13: How Big are Quantum States?](https://www.scottaaronson.com/democritus/lec13.html#:~:text=sort%20of%20the%20motivation%20for,it%2C%20or%20root%20it%20out)).

In stark contrast to the exponential growth of quantum state complexity, the **energy cost of adding an additional information unit** is relatively modest. Landauer’s principle in thermodynamics states that erasing one bit of information requires a minimum energy of $k_B T \ln 2$ (the Boltzmann constant times $T$ times ln2) ([
            The Landauer Principle: Re-Formulation of the Second Thermodynamics Law or a Step to Great Unification? - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC7514250/#:~:text=recording%2Ferasure%20of%20one%20bit%20of,the%20%E2%80%9Cinformational%E2%80%9D%20reformulation%20of%20thermodynamic)). This implies that doubling the number of distinguishable states (adding 1 bit, which doubles $W$) has only a logarithmic energy cost. *Experimentally*, this theoretical bound has been approached: a 2012 study demonstrated erasing a one-bit memory with heat dissipation close to the Landauer limit of $k_BT\ln 2$, confirming the link between information and thermodynamic cost ([Experimental verification of Landauer's principle linking information and thermodynamics - PubMed](https://pubmed.ncbi.nlm.nih.gov/22398556/#:~:text=In%201961%2C%20Rolf%20Landauer%20argued,well%20potential%2C%20we)) ([Experimental verification of Landauer's principle linking information and thermodynamics - PubMed](https://pubmed.ncbi.nlm.nih.gov/22398556/#:~:text=science%2C%20the%20erasure%20principle%20has,physical%20limit%20of%20irreversible%20computation)). In quantum systems, adding one more qubit or allowed state does not require exponentially more energy – it typically means incorporating one more particle or degree of freedom with roughly linear resource increase. Thus, the **information-to-energy ratio** skyrockets with system size. A small quantum system at room temperature can theoretically encode astronomically many states for each bit of thermodynamic cost.

*Supporting evidence:* The exponential scaling of quantum information is well-established in theory and has been indirectly confirmed by quantum algorithms that outperform their classical counterparts by exploiting large state spaces ([Superposition and entanglement - Quantum Inspire](https://www.quantum-inspire.com/kbase/superposition-and-entanglement/#:~:text=Quantum%20superposition%20is%20fundamentally%20different,of%20quantum%20states%20is%20exponential)). Landauer’s principle, initially controversial, has been validated in experiments linking one bit of entropy increase to a fixed tiny energy dissipation ([Experimental verification of Landauer's principle linking information and thermodynamics - PubMed](https://pubmed.ncbi.nlm.nih.gov/22398556/#:~:text=science%2C%20the%20erasure%20principle%20has,physical%20limit%20of%20irreversible%20computation)). This supports the idea that information storage is cheap energetically. 

*Contrasting viewpoint:* Some physicists note that although quantum states formally inhabit an exponentially large Hilbert space, not all that information is *accessible* or *useful* at once due to decoherence and measurement limits. Scott Aaronson has discussed the question of whether *n* qubits “behave like” $2^n$ classical bits or only *n* bits in practice ([PHYS771 Lecture 13: How Big are Quantum States?](https://www.scottaaronson.com/democritus/lec13.html#:~:text=sort%20of%20the%20motivation%20for,it%2C%20or%20root%20it%20out)). Furthermore, Landauer’s principle, while conceptually powerful, is essentially an expression of the second law of thermodynamics; some argue it is not a separate law but a restatement, and reaching the limit requires quasi-reversible processes that are hard to achieve ([
            The Landauer Principle: Re-Formulation of the Second Thermodynamics Law or a Step to Great Unification? - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC7514250/#:~:text=of%20a%20thermal%20reservoir%20used,quantum%20and%20relativity%20aspects%20of)). In summary, quantum mechanics offers a clear case of combinatorial information growth (via superposition and entanglement), and in principle the energy cost per additional quantum bit is minimal – though in practice, controlling many qubits without decoherence can demand significant engineering effort (cooling, error correction) that increases actual energy usage beyond the ideal logarithmic trend.

## AI Scaling Laws (Parameters vs Intelligence Growth)  

 ([image]()) *Performance of AI models on various benchmarks from 1998 to 2024, normalized with human-level performance marked by the black line. The past decade shows accelerated gains as deep learning models grew in size and dataset volume, achieving near-human or surpassing human-level performance on tasks like image recognition (ImageNet) and language understanding (GLUE) ([
            Large language models and brain-inspired general intelligence - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10630093/#:~:text=The%20emergence%20of%20large%20language,through%20collaboration%20with%20physical%20robotics)). These rapid improvements hint at the power of scaling: as models incorporate more parameters (and more training data), their capabilities often expand in a non-linear fashion – sometimes revealing **emergent abilities** not present in smaller models ([[2206.07682] Emergent Abilities of Large Language Models](https://arxiv.org/abs/2206.07682#:~:text=,of%20capabilities%20of%20language%20models)). However, this progress comes with steeply rising compute and energy costs; the largest models require enormous training power and the inference serving of these models can be computationally expensive ([
            Large language models and brain-inspired general intelligence - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10630093/#:~:text=One%20of%20the%20biggest%20concerns,US%20dollars%20when%20using%20cloud)).* 

In artificial intelligence, especially deep learning, increasing the number of parameters (neurons/synapses in a neural network) has yielded **disproportionate gains in performance** on complex tasks. Empirical *scaling laws* indicate that as we scale up model size *N* (and training data), the improvement in performance (e.g. lower error or higher score) often follows a power-law or better, rather than a simple linear trend ([[2001.08361] Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361#:~:text=,efficient%20training%20involves)). For instance, Kaplan *et al.* (2020) showed that language model cross-entropy loss decreases approximately as a power-law function of model parameters, data, and compute over a wide range – meaning each 10× increase in model size yielded a consistent boost in performance, with no sign of saturation in those ranges ([[2001.08361] Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361#:~:text=,efficient%20training%20involves)). Larger models also tend to be more *sample-efficient*, needing less data per unit performance ([[2001.08361] Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361#:~:text=govern%20the%20dependence%20of%20overfitting,and%20stopping%20significantly%20before%20convergence)), which suggests synergy: extra parameters are not just idle capacity, they actively make learning easier. Qualitatively, more parameters give the network a combinatorially larger space of possible internal representations/functions, enabling it to capture more intricate patterns. The **“emergent abilities”** observed in large language models exemplify this: abilities like multi-step reasoning or code generation suddenly appear once the model exceeds a certain complexity threshold, even though they were absent in smaller models ([[2206.07682] Emergent Abilities of Large Language Models](https://arxiv.org/abs/2206.07682#:~:text=,of%20capabilities%20of%20language%20models)). This suggests that scaling up doesn’t just add a bit more of the same ability – it can unlock entirely new capabilities (an *exponential* qualitative leap) that were not predictable by extrapolating linear trends from smaller models ([[2206.07682] Emergent Abilities of Large Language Models](https://arxiv.org/abs/2206.07682#:~:text=performance%20and%20sample%20efficiency%20on,of%20capabilities%20of%20language%20models)).

From an information-theoretic view, each additional parameter in a neural network can interact with existing parameters, creating combinatorially many possible configurations or features. In principle, if a network has *N* internal units, the number of distinct activation patterns those units can collectively represent is enormous (exponential in *N* for binary-like activations). While not all those patterns are useful, the capacity grows super-linearly as we add units. This is analogous to adding basis functions to an approximation – new functions don’t just add knowledge linearly; they can combine to represent a vastly larger family of functions. As a result, the “intelligence” or competence of AI models often scales faster than the raw count of parameters would suggest. We see this in benchmarks: a 175-billion parameter model (like GPT-3) can perform tasks that a 1-billion parameter model cannot, even when the smaller model is trained on the same data. The larger model’s breadth of learned representations enables generalization and problem-solving abilities that look **exponential** compared to the smaller one’s linear improvements. Indeed, recent large models have achieved feats like passing parts of the Turing test or expert exams, which required a leap in capability rather than an incremental tweak ([
            Large language models and brain-inspired general intelligence - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10630093/#:~:text=The%20emergence%20of%20large%20language,through%20collaboration%20with%20physical%20robotics)).

However, there are important **counterpoints** and practical limits. First, the improvements, while significant, often follow a **diminishing returns** trend in practice – typically a power-law with an exponent < 1. This means each doubling of model size yields a smaller incremental gain than the previous doubling (albeit still a gain) ([[2001.08361] Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361#:~:text=,efficient%20training%20involves)). For example, if performance scales as $N^{-0.5}$ (hypothetically), going from 1B to 10B parameters might reduce error a lot, but going from 10B to 100B yields a smaller reduction. Eventually, purely scaling parameters might hit a *scaling ceiling* for certain tasks ([AI's $100bn question: The scaling ceiling - Exponential View](https://www.exponentialview.co/p/can-scaling-scale#:~:text=AI%27s%20%24100bn%20question%3A%20The%20scaling,law%20describes%20capabilities%20will%20emerge)). There is debate over whether scaling alone will continue to produce “exponential” intelligence growth indefinitely. Some researchers argue that new algorithms or architectures (not just more units) will be needed to sustain improvements ([Neural Scaling Laws and the Limits of Intelligence - Medium](https://medium.com/@vector1210/neural-scaling-laws-and-the-limits-of-intelligence-3181e6317c82#:~:text=Neural%20Scaling%20Laws%20and%20the,force%20parameter%20growth)), to avoid simply brute-forcing with massive compute. Indeed, recent work (Hoffmann *et al.* 2022, “Chinchilla”) found that for a fixed compute budget, there’s an optimal balance of model size and data size – simply increasing parameters without enough data can be suboptimal, hinting that unlimited growth has constraints.

Secondly, unlike the idealized notion that adding one unit costs little energy, in AI the energy/computation cost grows roughly linearly or worse with model size. Training a state-of-the-art model requires huge clusters of GPUs for days or weeks, consuming megawatt-hours of energy. For instance, an analysis of GPT-3’s training estimated hundreds of thousands of dollars of cloud compute, and running many queries on such models continually incurs significant power usage ([
            Large language models and brain-inspired general intelligence - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10630093/#:~:text=One%20of%20the%20biggest%20concerns,US%20dollars%20when%20using%20cloud)). Thus, *each additional parameter* has a real energy and monetary cost during both training and inference, even if the *information gain* per parameter is high. The current trend shows **exponential compute growth** for achieving each new milestone in AI (often called an orthogonal “scaling law”): models are getting bigger and more data-hungry, and the total training compute is doubling faster than every 2 years (outpacing Moore’s Law) ([AI Model Scaling Isn't Over: It's Entering a New Era - AI Business](https://aibusiness.com/language-models/ai-model-scaling-isn-t-over-it-s-entering-a-new-era#:~:text=of%20compute%2C%20data%2C%20and%20time,This)). This raises concerns about sustainability. **In summary**, AI scaling demonstrates how adding more “brain power” in silicon yields more than linear improvements in capability (sometimes unexpected emergent leaps) – echoing combinatorial information scaling – but it also highlights practical energy limits, as each new era of models demands significantly more computational resources. Future research is focusing on making AI *more efficient* (getting more intelligence per parameter or per joule) so that we can continue to reap combinatorial gains without exponential energy consumption.

## Biological Intelligence and Neural Efficiency  
Biological brains provide a blueprint for *optimizing intelligence per unit energy*. The human brain contains on the order of $8\times10^{10}$ (80+ billion) neurons interconnected by an estimated $10^{14}$ (100 trillion) synapses ([A New Field of Neuroscience Aims to Map Connections in the Brain | Harvard Medical School](https://hms.harvard.edu/news/new-field-neuroscience-aims-map-connections-brain#:~:text=Many%20of%20us%20have%20seen,the%20human%20brain%20to%20fathom)). Each neuron is a relatively slow, modestly powered unit, but together these neurons form an extraordinarily complex network. Combinatorially, the number of distinct brain states or connectivity patterns is astronomically large – this massive state space underlies our cognitive capabilities. **Adding neurons** in a brain increases potential connectivity roughly combinatorially: each new neuron can form thousands of new connections with existing neurons, expanding the complexity of the network’s wiring options significantly. Thus, bigger brains (up to a point) can support more complex information processing not just by sheer neuron count, but by vastly richer *interaction patterns*. For example, the human cerebral cortex, by virtue of its many neurons and synapses, can encode and integrate information in ways that a smaller network (like a mouse brain) simply cannot, because there are far more possible circuits and activity patterns available. The *multiplicity* of pathways for signals in a large brain gives it a combinatorial edge in representational capacity and problem-solving.

Crucially, biological evolution has favored **neural efficiency** – maximizing cognitive output while minimizing energy input. The human brain runs on roughly **20 watts** of power (about the energy of a dim light bulb) ([
      Learning from the brain to make AI more energy-efficient
    ](https://www.humanbrainproject.eu/en/follow-hbp/news/2023/09/04/learning-brain-make-ai-more-energy-efficient/#:~:text=In%20contrast%20to%20power,if%20they%20were%20done%20artificially)), yet it performs tasks (vision, language, abstract thinking) that would require supercomputers consuming orders of magnitude more energy to even approach. One comparison noted that *trillions of operations* performed by the brain’s 80–100 billion neurons would **“require the power of a small hydroelectric plant”** if done on conventional hardware ([
      Learning from the brain to make AI more energy-efficient
    ](https://www.humanbrainproject.eu/en/follow-hbp/news/2023/09/04/learning-brain-make-ai-more-energy-efficient/#:~:text=In%20contrast%20to%20power,if%20they%20were%20done%20artificially)). The brain’s efficiency comes from both **hardware and software optimizations**: neurons are event-driven (spiking only as needed, rather than continuously burning energy), information is encoded sparsely (only a subset of neurons activate for any given stimulus), and synapses adapt to strengthen important pathways and weaken unnecessary ones, reducing wasteful activity. Over evolutionary timescales, brains have developed diverse neuron types and **selective connectivity patterns** that optimize information processing per calorie consumed ([
            Large language models and brain-inspired general intelligence - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10630093/#:~:text=A%20promising%20approach%20to%20solving,%E2%80%98child%20machine%E2%80%99%20as%20Turing%20once)). In other words, rather than randomly wiring up all possible connections (which would be combinatorially impossible and energetically costly), biological neural networks prune and tune their connections to maximize relevant complexity (useful computations) while minimizing redundancy or noise. During development, synaptic pruning eliminates excess connections, effectively *optimizing the network topology* for efficient function. The end result is a system that exploits the combinatorial richness of ~100 trillion synapses but operates on a very tight energy budget.

The **neural efficiency hypothesis** in cognitive science provides experimental support for this idea: it observes that more intelligent individuals tend to show *lower* brain activation (and presumably less energy use) for the same cognitive task, compared to less intelligent individuals ([Neural efficiency hypothesis - Wikipedia](https://en.wikipedia.org/wiki/Neural_efficiency_hypothesis#:~:text=Smart%20people%20like%20tough%20problems)). In functional brain imaging studies, people with higher IQ or greater expertise often exhibit *sparser, more focused* neural activity, indicating they’ve optimized their neural circuits to solve tasks with minimal wasted effort ([Neural efficiency hypothesis - Wikipedia](https://en.wikipedia.org/wiki/Neural_efficiency_hypothesis#:~:text=Smart%20people%20like%20tough%20problems)). This is essentially the brain leveraging its complex network in an *efficient* way – achieving the needed computation with fewer neurons firing, thereby saving energy. It echoes the point that raw “brain size” or power is not the sole determinant of intelligence; how effectively the brain uses its resources is key. For example, the human brain isn’t the largest in absolute size – elephants and whales have more neurons – but the human brain’s specific architecture (highly folded cortex, dense interconnected cortical columns, etc.) is thought to confer superior cognitive abilities per neuron. Evolution has effectively found a sweet spot where **each additional neuron** provides as much new computational capability as possible (through new connections and specialized circuits) without proportionally large increases in energy cost. The brain devotes about 20% of the body’s metabolic energy to itself, a significant investment, but it uses that energy *remarkably efficiently* to produce intelligent behavior.

*Supporting evidence:* Neuroscience research has quantified the brain’s efficiency. For instance, one study noted that the cortex spends far more energy on synaptic transmission (communication) than on neuron firing, implying the brain is optimized to invest energy in wiring that improves network communication efficiency ([Communication consumes 35 times more energy than computation ...](https://www.pnas.org/doi/10.1073/pnas.2008173118#:~:text=Communication%20consumes%2035%20times%20more,needed%20to%20predict%20synapse%20number)). As another example, the brain’s spontaneous activity and default network dynamics are organized in a way that may support predictive coding, reducing surprise and unnecessary energy expenditure. On a larger scale, animals that evolved greater intelligence (primates vs. other mammals) often did so not simply by greatly enlarging the brain (which has diminishing returns due to longer wiring and latency) but by **reorganizing neural circuits** and increasing the folding/area to pack more connectivity without huge increases in volume. This reflects an optimization of combinatorial connectivity. Experimental brain-machine interface studies also show that when neural circuits learn a task, they often become more efficient (neuronal responses sharpen or redundant neurons drop out) as the task becomes learned, indicating a drive toward minimal energy use for the required computation.

*Contrasting viewpoint:* There are physical and evolutionary limits to brain scaling. The brain’s 20 W budget, while efficient, also constrains its processing speed – neurons operate in milliseconds, far slower than nanosecond transistor switches, because faster signaling would consume more energy and produce more heat than the brain could dissipate. So the brain sacrifices raw speed for energy efficiency, relying on massive parallelism to compensate. Simply adding more neurons also faces diminishing returns: beyond a certain point, additional neurons might yield only marginal cognitive benefit if not organized properly, and would incur higher metabolic cost (e.g., large brains make childbirth difficult and require long development). Indeed, within humans, brain size correlates only modestly with intelligence ([The causal influence of brain size on human intelligence](https://pmc.ncbi.nlm.nih.gov/articles/PMC7440690/#:~:text=intelligence%20pmc,question%20of%20whether%20the)), suggesting that **architecture and efficiency** matter more. In evolutionary history, many species did not evolve bigger brains once a certain functional capacity was reached, presumably because the energy cost would outweigh the benefit in their niche. Human brains are at the upper end of feasible energy consumption for an organ – about 20% of total metabolism – indicating nature found a balance where further intelligence gains had to come from *efficient coding* and cultural learning rather than continually larger brains. In summary, biological intelligence exemplifies combinatorial complexity achieved through vast interconnectivity, all maintained on a tight energy budget via optimized neural processes.

## Physics, Entropy, and Energy Optimization  
At the most fundamental level, the relationship between combinatorial growth and logarithmic energy scaling is encapsulated in the physics of entropy and thermal information. **Entropy** ($S$) in statistical mechanics is defined by the famous Boltzmann formula $S = k_B \ln W$ ([Boltzmann's entropy formula - Wikipedia](https://en.wikipedia.org/wiki/Boltzmann%27s_entropy_formula#:~:text=Image%3A%20,1)), where $W$ is the number of microstates (configurations) consistent with a given macrostate. This equation directly connects a *combinatorial count* of states ($W$) with a logarithm – the natural logarithm (base *e*, Euler’s number) – yielding the entropy. In essence, if the number of possible states of a system grows exponentially, the entropy grows linearly (because of the log). For example, if you have two independent subsystems with $W_1$ and $W_2$ microstates, together they have $W_1 \times W_2$ microstates; entropy being logarithmic means $S_{\text{total}} = k_B \ln(W_1 W_2) = k_B \ln W_1 + k_B \ln W_2$, so entropy is extensive (additive) while $W$ multiplies. This mathematical fact underlies why adding a bit (doubling $W$) increases entropy by a constant $k_B \ln 2$ – the same principle we saw in Landauer’s limit. The use of the natural log (Euler’s number *e*) is natural in physics because it makes many relationships linear and easy to combine, reflecting how energy and probability scale.

**Boltzmann’s distribution** further shows how *e* (Euler’s number) arises in energy optimization. The probability that a system in thermal equilibrium occupies a state with energy $E_i$ is proportional to $\exp(-E_i/k_B T)$ ([Boltzmann distribution - Wikipedia](https://en.wikipedia.org/wiki/Boltzmann_distribution#:~:text=Image%3A%20%7B%5Cdisplaystyle%20p_%7Bi%7D%5Cpropto%20%5Cexp%20%5Cleft%28,kT%7D%7D%5Cright)). This exponential weighting (with base *e*) is the result of the system exploring many microstates while favoring lower energy ones – effectively balancing energy minimization against the combinatorial explosion of higher-energy states. The factor $e^{-E/k_BT}$ comes from maximizing entropy (the number of microstates $W$) for a given average energy – it is the solution that nature finds to allocate probabilities such that the free energy is minimized. In other words, the **most probable distribution** of states (the Boltzmann distribution) is one where the *logarithm* of the number of microstates (entropy) is maximized, subject to an energy constraint. The outcome is an exponential relation: for each additional unit of energy a state has, its probability drops by a factor of $e^{-1/k_BT}$, a gentle decay that still allows many higher-energy states to be partially occupied. This is an elegant demonstration of **energy optimization via combinatorial reasoning** – the form of the exponential ensures that the enormous number of higher-energy microstates collectively can compete with the fewer low-energy microstates in determining the system’s behavior, achieving a balance. The constant *e* emerges as the base of natural logarithms that makes these trade-offs work out smoothly.

Euler’s number also appears in the context of growth and learning processes. For example, in population growth or autocatalytic reactions, systems that grow proportionally to their current size exhibit exponential growth $N(t) = N(0)e^{rt}$, where *e* again is the base. Such growth is the continuous analog of combinatorial multiplication. In neural networks (both artificial and biological), activation functions like the logistic sigmoid $1/(1+e^{-x})$ are used precisely because they naturally arise from differential equations and offer a convenient saturation behavior. The prevalence of *e* in these contexts is not a coincidence – it is the number that links continuous change with multiplicative processes. *Intelligence formation* in a physical sense can be viewed through the lens of self-organization and entropy. Systems that *self-organize* to a higher level of complexity (like life or intelligent structures) do so by taking in energy and exporting entropy to the environment, thus staying within the second law of thermodynamics. 

Recent theoretical work by Jeremy England proposes that under certain conditions, matter will spontaneously reconfigure itself to better dissipate energy – effectively *maximizing entropy production* – which could drive the formation of lifelike or intelligent structures. England derived a formula suggesting that when a group of atoms is driven by an external energy source (like sunlight) in contact with a heat bath, it will often rearrange to dissipate increasingly more energy as heat ([A New Physics Theory of Life | Quanta Magazine](https://www.quantamagazine.org/a-new-thermodynamics-theory-of-the-origin-of-life-20140122/#:~:text=indicates%20that%20when%20a%20group,physical%20attribute%20associated%20with%20life)). In this view, structures that we associate with life or intelligence might arise naturally as those that are most efficient at using available energy to generate entropy. Living organisms, for example, absorb highly ordered energy (sunlight or food) and dissipate it as low-grade heat, all the while maintaining internal complexity. As England provocatively put it, given enough time and energy, *“you start with a random clump of atoms, and if you shine light on it for long enough, you get a plant”* ([A New Physics Theory of Life | Quanta Magazine](https://www.quantamagazine.org/a-new-thermodynamics-theory-of-the-origin-of-life-20140122/#:~:text=%E2%80%9CYou%20start%20with%20a%20random,get%20a%20plant%2C%E2%80%9D%20England%20said)). This is a bold hypothesis linking **entropy and intelligence (or at least complexity) formation**: intelligence could be seen as a strategy to dissipate energy gradients more effectively (e.g., a brain allows an organism to find and utilize energy sources more efficiently, thereby increasing overall entropy production of its environment). It’s an enticing connection between the combinatorial possibilities of matter and the thermodynamic drive for energy dispersal.

*Experimental support and parallels:* While England’s specific theory of “dissipation-driven adaptation” is still being tested, it aligns with known principles like **Prigogine’s dissipative structures**, where systems far from equilibrium self-organize (e.g. convection cells, chemical oscillators) to increase entropy production. We see in evolution that life has continually found ways to use more energy (consider the increase in metabolic rates and complexity from single-celled organisms up to mammals with large brains). This suggests a trend where complexity emerges hand-in-hand with the exploitation of energy gradients. Even on the planetary scale, some have argued that the biosphere as a whole increases the Earth’s entropy production (by absorbing sunlight and emitting infrared radiation) more than would a bare rock, indicating life’s emergence boosted entropy production – a possible thermodynamic *incentive* for complexity. The mathematics of these phenomena inevitably involves exponentials and logarithms, because they deal with large numbers of possibilities and optimizing distributions.

*Contrasting perspective:* Not everyone agrees that entropy maximization is a sufficient or primary explanation for intelligence. England’s work, for example, though **theoretically valid**, is considered speculative regarding life’s origin ([A New Physics Theory of Life | Quanta Magazine](https://www.quantamagazine.org/a-new-thermodynamics-theory-of-the-origin-of-life-20140122/#:~:text=Others%2C%20such%20as%20Eugene%20Shakhnovich%2C,to%20life%20phenomena%2C%E2%80%9D%20Shakhnovich%20said)). Critics point out that just because a system could dissipate more energy by becoming more complex doesn’t mean it easily will – there is still a huge combinatorial search space and many local optima. Other factors (like specific chemistry, historical contingency, or genetic variation and selection in biological evolution) are critical. Moreover, intelligence in the human sense involves memory, foresight, and goal-directed behavior, which are abstract concepts not obviously reducible to entropy maximization alone. However, **no physical law is violated by the emergence of intelligence**; it obeys thermodynamics by consuming energy. A brain, for instance, generates a lot of heat (entropy) while it thinks. What the physics perspective adds is an understanding that complex order can spontaneously form in open systems because the *number of ways* (microstates) to dissipate energy is greater when the system is structured a certain way. The interplay of combinatorics and energy is exemplified by the fact that a highly ordered structure (low entropy locally) can enable many more entropy-increasing interactions with the environment than a random structure could. 

In summary, physics shows us that whenever we deal with **large numbers of configurations**, logarithms and exponentials (Euler’s number *e*) naturally enter the formulas. Combinatorial growth in possibilities is tamed by taking logs, yielding linear measures like entropy that connect directly to energy. Intelligence and complexity can be seen as part of this story: they are allowed (and perhaps encouraged) by the second law as long as they contribute to net entropy growth of the universe. The efficiencies observed – whether in bits per joule in computing, bits per neuron in brains, or states per energy in quantum systems – all reflect the profound fact that adding discrete units multiplies state count, while costs add. Nature often finds ways to leverage this: building up high complexity with relatively low incremental energy cost, by operating near thermodynamic optimality. Yet, harnessing the full combinatorial potential requires structure and order (algorithms, architectures, evolutionary adaptations), and there are practical limits to how efficiently any system can use energy. The contrasting needs of *exploration* (many possibilities) and *exploitation* (energy optimization) ultimately shape the emergence of intelligent behavior under physical constraints.

## Conclusion  
Across quantum physics, artificial and biological intelligence, and thermodynamic systems, we observe a common theme: **combinatorial explosion of information or state possibilities with each added unit, versus sub-exponential (often logarithmic or linear) increase in energy requirements.** This disparity is what makes increasing complexity possible. In quantum mechanics, a few additional qubits massively expand computational state space with only a linear resource cost in qubit count. In AI, scaling up parameters opens up a vastly richer set of representable functions, yielding disproportionately higher capabilities – albeit with growing computational costs that we strive to manage. The human brain shows how evolution exploited combinatorial connectivity of billions of neurons to achieve remarkable intelligence, all while keeping energy usage extremely low via optimized processes. In thermodynamics, the mathematics of entropy and the prevalence of $e$ and $\ln$ demonstrate how nature counts combinations and balances them against energy in a logarithmic way. 

Fundamentally, these cases highlight that **complex, intelligent systems can emerge and operate without prohibitive energy costs** because the *architectures* (be it quantum superposition, deep network connectivity, cortical organization, or statistical distributions) allow exponential information growth on top of additive energy budgets. The **trade-off** is that such systems typically require careful organization to be useful: quantum coherence must be preserved to exploit $2^n$ states, AI models need the right training and structure to turn billions of parameters into general intelligence, brains need to wire neurons effectively to make use of 100 trillion synapses, and physical systems need the right conditions to self-organize. Where those organizational principles are in place, the combinatorial richness can be harnessed for order and intelligence. 

In supporting these ideas, we cited theoretical work (e.g. Landauer’s limit, scaling law equations, entropy formulas) that frames the limits and potentials, as well as experimental evidence (bit erasure experiments, AI benchmark performances, neuroscience findings) that show these principles at work in reality. There are also cautions: diminishing returns in AI model scaling, biological constraints on brain size, decoherence in quantum systems, and the open question of how exactly physics transitions to life and mind. These remind us that **combinatorial growth** is a double-edged sword – it offers immense possibility, but controlling and *utilizing* that possibility is the domain of structure, design, and sometimes luck. Nonetheless, the overarching insight is empowering: we do not need exponentially more energy to achieve exponentially more complexity. By leveraging clever architectures and the math of information, we can add knowledge, computational power, or intelligence at sub-linear energy cost per unit – a principle that has driven progress from the quantum transistor to the human brain, and perhaps governs the very emergence of organized complexity in the universe ([A New Physics Theory of Life | Quanta Magazine](https://www.quantamagazine.org/a-new-thermodynamics-theory-of-the-origin-of-life-20140122/#:~:text=indicates%20that%20when%20a%20group,physical%20attribute%20associated%20with%20life)).

**Sources:**

1. Landauer’s principle – erasing one bit requires $k_B T \ln 2$ energy (theoretical and experimental support) ([
            The Landauer Principle: Re-Formulation of the Second Thermodynamics Law or a Step to Great Unification? - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC7514250/#:~:text=recording%2Ferasure%20of%20one%20bit%20of,the%20%E2%80%9Cinformational%E2%80%9D%20reformulation%20of%20thermodynamic)) ([Experimental verification of Landauer's principle linking information and thermodynamics - PubMed](https://pubmed.ncbi.nlm.nih.gov/22398556/#:~:text=science%2C%20the%20erasure%20principle%20has,physical%20limit%20of%20irreversible%20computation)).  
2. Quantum superposition – *n* qubits span $2^n$ states (exponential growth of information) ([Superposition and entanglement - Quantum Inspire](https://www.quantum-inspire.com/kbase/superposition-and-entanglement/#:~:text=Quantum%20superposition%20is%20fundamentally%20different,of%20quantum%20states%20is%20exponential)).  
3. AI scaling laws – performance improves non-linearly (power-law) with model size; emergent abilities in large models ([[2001.08361] Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361#:~:text=,efficient%20training%20involves)) ([[2206.07682] Emergent Abilities of Large Language Models](https://arxiv.org/abs/2206.07682#:~:text=,of%20capabilities%20of%20language%20models)).  
4. AI scaling limits – diminishing returns and high compute costs noted for very large models ([[2001.08361] Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361#:~:text=,efficient%20training%20involves)) ([
            Large language models and brain-inspired general intelligence - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10630093/#:~:text=One%20of%20the%20biggest%20concerns,US%20dollars%20when%20using%20cloud)).  
5. Brain efficiency – 86B neurons & 100T synapses run on ~20 W, far outperforming computers per watt ([
      Learning from the brain to make AI more energy-efficient
    ](https://www.humanbrainproject.eu/en/follow-hbp/news/2023/09/04/learning-brain-make-ai-more-energy-efficient/#:~:text=In%20contrast%20to%20power,if%20they%20were%20done%20artificially)) ([
            Large language models and brain-inspired general intelligence - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10630093/#:~:text=A%20promising%20approach%20to%20solving,%E2%80%98child%20machine%E2%80%99%20as%20Turing%20once)).  
6. Neural efficiency hypothesis – higher intelligence brains use less activation/energy for tasks ([Neural efficiency hypothesis - Wikipedia](https://en.wikipedia.org/wiki/Neural_efficiency_hypothesis#:~:text=Smart%20people%20like%20tough%20problems)).  
7. Entropy and combinatorics – $S = k_B \ln W$ links log of microstates to energy/entropy ([Boltzmann's entropy formula - Wikipedia](https://en.wikipedia.org/wiki/Boltzmann%27s_entropy_formula#:~:text=Image%3A%20,1)).  
8. Boltzmann distribution – probability ∝ $e^{-E/kT}$ shows exponential energy optimization ([Boltzmann distribution - Wikipedia](https://en.wikipedia.org/wiki/Boltzmann_distribution#:~:text=Image%3A%20%7B%5Cdisplaystyle%20p_%7Bi%7D%5Cpropto%20%5Cexp%20%5Cleft%28,kT%7D%7D%5Cright)).  
9. England’s dissipative adaptation theory – matter reorganizes to dissipate more energy (controversial idea tying entropy to life’s emergence) ([A New Physics Theory of Life | Quanta Magazine](https://www.quantamagazine.org/a-new-thermodynamics-theory-of-the-origin-of-life-20140122/#:~:text=indicates%20that%20when%20a%20group,physical%20attribute%20associated%20with%20life)) ([A New Physics Theory of Life | Quanta Magazine](https://www.quantamagazine.org/a-new-thermodynamics-theory-of-the-origin-of-life-20140122/#:~:text=Others%2C%20such%20as%20Eugene%20Shakhnovich%2C,to%20life%20phenomena%2C%E2%80%9D%20Shakhnovich%20said)).
